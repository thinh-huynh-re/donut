resume_from_checkpoint_path: null # only used for resume_from_checkpoint option in PL
result_path: "./result"
pretrained_model_name_or_path: "naver-clova-ix/donut-base" # loading a pre-trained model (from moldehub or path)
tokenizer_name_or_path: "xlm-roberta-base" # loading a pre-trained model (from moldehub or path)
dataset_name_or_paths: # loading datasets (from moldehub or path)
  # - "naver-clova-ix/cord-v2"
  # - "naver-clova-ix/synthdog-en"
  - "extracted_datasets/synthdog-en"
  # - "naver-clova-ix/synthdog-ja"
  # - "naver-clova-ix/synthdog-ko"
splits:
  - ["train", "validation"]
  # - ["train", "validation"]
  # - ["train", "validation"]
  # - ["train", "validation"]
task_start_tokens: ["<s_pretrain>"]
sort_json_key: False # cord dataset is preprocessed, and publicly available at https://huggingface.co/datasets/naver-clova-ix/cord-v2
train_batch_sizes: [3]
val_batch_sizes: [1]
input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
max_length: 768
align_long_axis: False
num_nodes: 1
devices: [1, 2]
seed: 2022
lr: 1e-4
warmup_steps: 375000 # 1004/2*30/10, 10% = num_training_samples_per_epoch / num_workers * max_epochs / 10
num_training_samples_per_epoch: 250000
max_epochs: 30
max_steps: -1
num_workers: 3
val_check_interval: 1.0
check_val_every_n_epoch: 1
gradient_clip_val: 1.0
verbose: True
wandb: True
local_files_only: True
preload: False
